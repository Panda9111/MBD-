##functions 
# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}


#optional
#install.packages(xlsx)
#install.packages(tidyverse)
#install.packages(data.table)
#install.packages(reshape2)
#install.packages(modelr)
#install.packages("caret")
#setwd()



# Hadley Wickham, from new zealand, the creator of tidyverse which use very 
#popular libraries such as data visualization with ggplot, data cleaning dplyr
#string for string and regular expression manipulation 
library(tidyverse)
library(broom)
#for visualization of correlation 
library(reshape2)
#for model assesment
library(modelr)
library(caret)
library(xlsx)


database_kyc <- read.csv("RBA_KYC_Accounts_ALL_Ids.csv")

#this used with there is a problem with importing the csv file 
#database_kyc <- read.csv2("RBA_KYC_Accounts_ALL_Ids.csv",sep = ";")




############ Explanatory data analysis  ######################

#to see how the data frame looks like
head(database_kyc)
# to see the structure of the data set 
str(database_kyc)

#type of TYPE_OF_DEP,GENDER,companyType, profession, age_in_year is a character type
database_kyc$TYPE_OF_DEP <- as.numeric(database_kyc$TYPE_OF_DEP)
database_kyc$GENDER <- as.numeric(database_kyc$GENDER)
database_kyc$companyType <- as.numeric(database_kyc$companyType)
database_kyc$profession <- as.numeric(database_kyc$profession )
database_kyc$age_in_year <- as.numeric(database_kyc$age_in_year)


#apply to all columns, we can see we have some columns will all NAs
sapply(database_kyc,function(x) sum(is.na(x)))

#we can drop the following columns :  score_card_Desc,birthCountry ,legal_Stat_desc 
database_kyc <- database_kyc %>% select(-score_card_Desc,-birthCountry ,-legal_Stat_desc )

#we noticed that there were a lot of NULLs as string we can convert them in to NAs
# major problems
#we can already using the structure function that there are lot off NULLs 
#IMPORTANT NOTE NULLs are in the form of STRINGs 
#NAs in the data set we must able to
#filtering for NAs to see how many NAs 

#turn or NULLs into NAs
database_kyc[database_kyc == "NULL"] <- NA

sapply(database_kyc,function(x) sum(is.na(x)))


####apply count to all functions that are character columns  ### 
#count function to see the problematic data set example ,
#make a function lol

count(database_kyc,org_code) # remove all columns are the same
count(database_kyc,branchCode)
count(database_kyc,customerType)
count(database_kyc, onboarding)
count(database_kyc, residentStatus)
count(database_kyc, residentCountry) # residentCountry a lot of NA
count(database_kyc,nationalityOriginal) #nationalityOriginal 3439
count(database_kyc,extraNationality) #extraNationality <NA> 223075
count(database_kyc,age_in_year) #age_in_year  417 NA the same as dateOfBirth (remove col)
count(database_kyc,birthPlace) #birthPlace  too many error delete column 
count(database_kyc, profession) #profession NA 145120
count(database_kyc, companyType) # companyType NA 110294
count(database_kyc, giinCode) #giinCOde remove column
count(database_kyc,lastUpdate) #lastUpdate  ### remove the column 
count(database_kyc,status) #status  ##all items are the same 
count(database_kyc,ledgerCode)
count(database_kyc,accountCurrency)
count(database_kyc,IsBlackListed) #IsBlackListed remove table all the same values 
count(database_kyc,CUS_CLASS)
count(database_kyc,ECO_SEC)
count(database_kyc,TYPE_OF_DEP) #TYPE_OF_DEP remove the x 
count(database_kyc, GENDER) #GENDER <NA> 120511 
count(database_kyc,LEGAL_STA_CODE) ## remove the  <NA>   2616
#date_of_assessment not sure 
count(database_kyc,rbaGradeAbrv) # rba_grade_desc and rbaGradeAbrv the same 
count(database_kyc,rba_grade_desc)
count(database_kyc,score_card)

#for users to visualize the columns , change x and fill
ggplot(database_kyc,aes(x=GENDER, fill = GENDER)) +
  stat_count(width = 0.5) +
  theme_classic()

#drop the columns that are not needed 
#org_code(all are 1 )
#birthCountry (all NAs),
#birthPlace (to be checked)
#lastUpdate(all nulls ) 
#legal_Stat_desc(all NAs)
#score_card_Desc(all Nas )
#IsBlackListed(all values are 0s) 
#date_of_assingment (everything is 2019-01-24)
#dateofbirth already has a similar value which is age_in_year)

database_dropped <- database_kyc %>% select(-org_code,-residentCountry,
                                            -`extraNationality`,-dateOfBirth,
                                            -birthPlace,-giinCode,
                                            -lastUpdate,-status,
                                            -IsBlackListed,-rbaGradeAbrv,-date_of_assessment)
str(database_dropped)

####remove all NAs #  (to be discussed by group )
cleaned_database <- na.omit(database_dropped)


#to see if the x in the TYPE_OF_DEP was removed
count(cleaned_database,TYPE_OF_DEP)


###########Recoding optional #############

#mutate(gender = factor(gender, levels = c("Male","Female")))

########now to see the correlation of columns with numeric values#######
str(cleaned_database)    
mydata <- cleaned_database %>% select(avg_of_wd_90_days,number_of_wd_90_days,
                                      avg_of_cash_wd_90_days,number_of_cash_wd_90_days,
                                      avg_of_dep_90_days,number_of_deposit_90_days,avg_cash_deposit_90_days,
                                      number_of_cash_dep_90_days,avg_last_90_days,avg_last_30_days,avg_last_10_days)

cormat <- round(cor(mydata),2)
#reshapes the data
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 7, hjust = 1))+
  coord_fixed()
# Print the heatmap
print(ggheatmap)

# we want to see the correlated assets
ggheatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5))

#we can the throw how those highly correlated variables 
cleaned_database_less_var <- cleaned_database %>% select(-avg_last_30_days,
                                                         -avg_last_10_days,
                                                         -avg_cash_deposit_90_days)


### other databases  for left_join 
database_Profession <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 1 ) %>% select(-Description)
database_Ledgers <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 3 ) %>% select(-dsc)
database_Eco_sec <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 5 ) %>% select(-dsc)
database_Type_Dep<- read.xlsx("RBA Metadata.xlsx",sheetIndex = 6 ) %>% select(-dsc)
database_legal_sta_code <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 7 ) %>% select(-dsc)
database_countries <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 8 ) %>% select(-dsc)
database_Currencies <- read.xlsx("RBA Metadata.xlsx",sheetIndex = 9 ) %>% select(-dsc)

database_kyc$TYPE_OF_DEP <- as.numeric(database_kyc$TYPE_OF_DEP)
database_kyc$GENDER <- as.numeric(database_kyc$GENDER)
database_kyc$companyType <- as.numeric(database_kyc$companyType)
database_kyc$age_in_year <- as.numeric(database_kyc$age_in_year)
database_Profession$Value <- as.numeric(database_Profession$Value)

cleaned_database_less_var <- cleaned_database_less_var %>% left_join(database_Profession, by = c('profession'= 'Value')) %>%
                                            left_join(database_Ledgers, by = c('ledgerCode'= 'value')) %>%
                                            left_join(database_Eco_sec, by = c('ECO_SEC'= 'value')) %>%
                                            left_join(database_Type_Dep, by = c('TYPE_OF_DEP'= 'value')) %>%
                                            left_join(database_legal_sta_code, by = c('LEGAL_STA_CODE'= 'value')) %>%
                                            left_join(database_countries, by = c('nationalityOriginal'= 'value')) %>%
                                            left_join(database_Currencies, by = c('accountCurrency'= 'value'))
                                        
                                            
  


#now we choose the variables we want 
str(cleaned_database_less_var)



#Then, develop one Regression model using rbaValue as target variable. 

####add creation date change into as.date()

lm_model <- lm(rbaValue ~ jointAccount + rba_grade_desc + avg_last_90_days+
                 GENDER + ECO_SEC + avg_of_cash_wd_90_days +
                 branchCode+ LEGAL_STA_CODE +as.Date(creationDate)
               ,data = cleaned_database_less_var )

#to get the summary statistics of the model
summary(lm_model)
# to get the more advance details fort he linear model
augment(lm_model)
#RMSE and mape 
rmse(lm_model,cleaned_database_less_var)
mape(lm_model,cleaned_database_less_var)
mae(lm_model,cleaned_database_less_var)
#delete some useless variables
#here we are deleting variables with h p values



# proffesor regression model
#linear_reg() %>% set_engine("lm") %>% fit(hours_per_week ~ education_num + age + gender, data = train)

#################logistic regression ###################
#converting categorical variables into 1 or 0 
count(cleaned_database_less_var,rba_grade_desc)
## recode 
#cleaned_database_log <- cleaned_database_less_var %>% mutate(rba_grade_desc = recode(rba_grade_desc, 
#                                                                                     "Low"=0,
#                                                                                     "Medium"=1))

set.seed(3456)
trainIndex <- createDataPartition(cleaned_database_less_var$rba_grade_desc, p = .8,
                                  list = FALSE,
                                  times = 1)
Train <- cleaned_database_less_var[trainIndex,]
Test <- cleaned_database_less_var[-trainIndex,]


count(Train,rba_grade_desc)
## recode 
cleaned_database_log_Train <- Train %>% mutate(rba_grade_desc = recode(rba_grade_desc, 
                                                                       "Low"=0,
                                                                       "Medium"=1))
cleaned_database_log_Test<- Test %>% mutate(rba_grade_desc = recode(rba_grade_desc, 
                                                                    "Low"=0,
                                                                    "Medium"=1))
cleaned_database_log_Train <- Train %>% mutate(score_card = recode(score_card, 
                                                                   "E-Existing"=0,
                                                                   "E-NEW"=1,
                                                                   "I-Existing"=2,
                                                                   "I-New"=3))
cleaned_database_log_Test <- Test %>% mutate(score_card = recode(score_card, 
                                                                 "E-Existing"=0,
                                                                 "E-NEW"=1,
                                                                 "I-Existing"=2,
                                                                 "I-New"=3))




#changing the to be predicted value to factor
cleaned_database_log_Train$rba_grade_desc<-as.factor(cleaned_database_log_Train$rba_grade_desc)
#changing the to be predicted value to factor
cleaned_database_log_Test$rba_grade_desc<-as.factor(cleaned_database_log_Test$rba_grade_desc)

class(cleaned_database_log_Train$rba_grade_desc)
class(cleaned_database_log_Test$rba_grade_desc)

log_model <- glm(rba_grade_desc ~ jointAccount + avg_last_90_days+ score_card+
                   GENDER + avg_of_cash_wd_90_days +
                   branchCode+ LEGAL_STA_CODE, family=binomial(link='logit'), data=cleaned_database_log_Train)
summary(log_model)
anova(log_model, test="Chisq")

#predicting the model on the test set
fitted_results <- predict(log_model,cleaned_database_log_Test,type='response')
hist(fitted_results)
fitted_results <- ifelse(fitted_results > 0.1,1,0)
plot(fitted_results)

#Creating confusion Matrix
table_mat <- table(cleaned_database_log_Test$rba_grade_desc, fitted_results > 0.1)
table_mat

#Calculate Accuracy
accuracyTest = sum(diag(table_mat)) / sum(table_mat)
accuracyTest

#calculate Precision
precision <- function(matrix) {
  # True positive
  tp <- matrix[2, 2]
  # False positive
  fp <- matrix[1, 2]
  return (tp/(tp+fp))
}

prec = precision(table_mat)
prec

#calculate Recall
recall <- function(matrix) {
  # true positive
  tp <- matrix[2, 2]
  # false positive
  fn <- matrix[2, 1]
  return (tp/(tp+fn))
}

rec = recall(table_mat)
rec

#Calculate Specificity
specifity <- function(matrix) {
  # true positive
  tn <- matrix[1, 1]
  # false positive
  fp <- matrix[1, 2]
  return (tn/(tn+fp))
}

spe = specifity(table_mat)
spe

#Calculate F1 Score
y_pred <- fitted_results
class(y_pred)
cleaned_database_log_Test<- Test %>% mutate(rba_grade_desc = recode(rba_grade_desc, 
                                                                    "Low"=0,
                                                                    "Medium"=1))
y_true <- cleaned_database_log_Test$rba_grade_desc
class(y_true)
library(MLmetrics)
F1_Score(y_true, y_pred, positive = NULL)


#ROC Curve
p  <- predict(log_model,cleaned_database_log_Train,type='response')
library(ROCR)
pr <- prediction(p, cleaned_database_log_Train$rba_grade_desc)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

#AOC
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc



